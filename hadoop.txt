Here‚Äôs a **complete, step-by-step guide** to install and configure **Hadoop in Ubuntu** ‚Äî for both **single-node (pseudo-distributed)** and **multi-node (fully distributed)** clusters.
I‚Äôll explain clearly with commands and config file edits ‚úÖ

---

## üß© PREREQUISITES

**System requirements:**

* Ubuntu 20.04 / 22.04 (recommended)
* At least 4 GB RAM
* Java 8 or 11 installed
* SSH enabled

---

# üß† PART 1: SINGLE NODE HADOOP CLUSTER (Pseudo-Distributed Mode)

---

## üîπ Step 1: Update and Install Java

```bash
sudo apt update && sudo apt upgrade -y
sudo apt install openjdk-11-jdk -y
java -version
```

‚úÖ Output should show Java 11.

---

## üîπ Step 2: Create a Hadoop User

```bash
sudo adduser hadoop
sudo usermod -aG sudo hadoop
su - hadoop
```

---

## üîπ Step 3: Setup SSH (for Hadoop daemons communication)

```bash
ssh-keygen -t rsa -P ""
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 0600 ~/.ssh/authorized_keys
ssh localhost
```

‚úÖ You should log in without password.

---

## üîπ Step 4: Download and Extract Hadoop

```bash
cd /opt
sudo wget https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz
sudo tar -xzvf hadoop-3.3.6.tar.gz
sudo mv hadoop-3.3.6 /usr/local/hadoop
sudo chown -R hadoop:hadoop /usr/local/hadoop
```

---

## üîπ Step 5: Set Environment Variables

Edit `.bashrc` file:

```bash
nano ~/.bashrc
```

Add these lines at the end:

```bash
export HADOOP_HOME=/usr/local/hadoop
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
```

Apply:

```bash
source ~/.bashrc
```

---

## üîπ Step 6: Configure Hadoop Files

Go to config directory:

```bash
cd $HADOOP_HOME/etc/hadoop
```

### 1Ô∏è‚É£ `hadoop-env.sh`

```bash
nano hadoop-env.sh
```

Set:

```bash
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
```

---

### 2Ô∏è‚É£ `core-site.xml`

```bash
nano core-site.xml
```

Add:

```xml
<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://localhost:9000</value>
  </property>
</configuration>
```

---

### 3Ô∏è‚É£ `hdfs-site.xml`

```bash
nano hdfs-site.xml
```

Add:

```xml
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>1</value>
  </property>
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>file:/usr/local/hadoop/tmp/dfs/name</value>
  </property>
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>file:/usr/local/hadoop/tmp/dfs/data</value>
  </property>
</configuration>
```

---

### 4Ô∏è‚É£ `mapred-site.xml`

```bash
cp mapred-site.xml.template mapred-site.xml
nano mapred-site.xml
```

Add:

```xml
<configuration>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
</configuration>
```

---

### 5Ô∏è‚É£ `yarn-site.xml`

```bash
nano yarn-site.xml
```

Add:

```xml
<configuration>
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>
</configuration>
```

---

## üîπ Step 7: Format the Namenode

```bash
hdfs namenode -format
```

‚úÖ Output should show ‚Äú**Storage directory has been successfully formatted**‚Äù.

---

## üîπ Step 8: Start Hadoop Daemons

```bash
start-dfs.sh
start-yarn.sh
```

Check running daemons:

```bash
jps
```

‚úÖ Should see:

```
NameNode
DataNode
ResourceManager
NodeManager
SecondaryNameNode
```

---

## üîπ Step 9: Access Hadoop Web UI

| Service         | URL                                            |
| --------------- | ---------------------------------------------- |
| NameNode        | [http://localhost:9870](http://localhost:9870) |
| ResourceManager | [http://localhost:8088](http://localhost:8088) |

---

## üîπ Step 10: Test HDFS

```bash
hdfs dfs -mkdir /user
hdfs dfs -mkdir /user/hadoop
hdfs dfs -put $HADOOP_HOME/etc/hadoop/*.xml /user/hadoop
hdfs dfs -ls /user/hadoop
```

‚úÖ You should see the uploaded files listed.

---

# üñ•Ô∏è PART 2: MULTI-NODE HADOOP CLUSTER (Fully Distributed Mode)

---

## ‚öôÔ∏è Setup Overview

| Node   | Role                       | Example IP    |
| ------ | -------------------------- | ------------- |
| Master | NameNode + ResourceManager | 192.168.1.100 |
| Slave1 | DataNode + NodeManager     | 192.168.1.101 |
| Slave2 | DataNode + NodeManager     | 192.168.1.102 |

---

## üîπ Step 1: Repeat Installation

Do **Steps 1‚Äì5** (Java, SSH, Hadoop install, environment variables) on **all machines** (master and slaves).

---

## üîπ Step 2: Configure SSH from Master to Slaves

On **master node**:

```bash
ssh-keygen -t rsa -P ""
ssh-copy-id hadoop@slave1
ssh-copy-id hadoop@slave2
```

Test:

```bash
ssh slave1
ssh slave2
```

‚úÖ You should log in without password.

---

## üîπ Step 3: Configure Hadoop Files on Master

All Hadoop config files should be like the **single-node setup**, except:

### `core-site.xml`

```xml
<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://master:9000</value>
  </property>
</configuration>
```

---

### `hdfs-site.xml`

```xml
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>2</value>
  </property>
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>file:/usr/local/hadoop/tmp/dfs/name</value>
  </property>
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>file:/usr/local/hadoop/tmp/dfs/data</value>
  </property>
</configuration>
```

---

### `workers` file

```bash
nano $HADOOP_HOME/etc/hadoop/workers
```

Add:

```
master
slave1
slave2
```

---

### `yarn-site.xml`

```xml
<configuration>
  <property>
    <name>yarn.resourcemanager.hostname</name>
    <value>master</value>
  </property>
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>
</configuration>
```

---

## üîπ Step 4: Copy Configs to Slaves

From master:

```bash
scp -r $HADOOP_HOME/etc/hadoop hadoop@slave1:$HADOOP_HOME/etc/
scp -r $HADOOP_HOME/etc/hadoop hadoop@slave2:$HADOOP_HOME/etc/
```

---

## üîπ Step 5: Format HDFS on Master

```bash
hdfs namenode -format
```

---

## üîπ Step 6: Start Cluster from Master

```bash
start-dfs.sh
start-yarn.sh
```

Check:

```bash
jps
```

‚úÖ On Master ‚Üí NameNode, SecondaryNameNode, ResourceManager
‚úÖ On Slaves ‚Üí DataNode, NodeManager

---

## üîπ Step 7: Verify Web UI

| Component       | URL                                      |
| --------------- | ---------------------------------------- |
| NameNode        | [http://master:9870](http://master:9870) |
| ResourceManager | [http://master:8088](http://master:8088) |

---

‚úÖ **Your Hadoop multi-node cluster is running!**

---

Would you like me to give a **ready-to-run script (.sh)** that automates the single-node setup (with all environment variables and configs)?
It will let you install Hadoop in 2‚Äì3 minutes on a fresh Ubuntu system.
